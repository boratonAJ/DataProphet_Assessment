{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Precipitation Forecast Task\n",
    "\n",
    "Download this zip file, containing historical daily precipitation maps for the continental United States (excluding the state of Alaska). Your task is to train a model to take any number (determined by you) of daily precipitation maps as input, and generate precipitation forecast maps for one week (7 days) into the future.\n",
    "\n",
    "To complete this task, you may select any model architecture you deem appropriate. Include your python3 training and evaluation code, as well as a test application with a basic user interface as part of your submission. The interface does not have to include any graphical components, but must accept a list of input images, and save the resulting forecast maps as images to the current directory. Also include the generated forecast maps for one week after the last available date in the data, as on the date of your submission (the data is updated daily, but available information may lag by a few days).\n",
    "\n",
    "Write a report detailing your approach, and address (and justify as necessary) at least the following points in any order:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Definition Framework\n",
    "\n",
    "The steps I apply in solving the task is by understanding the problem. The challenge is to train a model that take images of daily precipitation maps as input, and generate precipitation forecast maps for one week (7 days) into the future. According to Tom Mitchell, machine learning is a computer program that learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. Therefore, this program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "This task assumes we have a Python SciPy environment installed as well as Python 3. The task assumes we also have Keras v2.0 or higher installed with either the TensorFlow or Theano backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librariesfrom PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "import os,cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import TimeDistributed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Model Evaluation\n",
    "\n",
    "This section of the task describes data preparation and model evaluation used in this tutorial\n",
    "\n",
    "#### Data Split\n",
    "\n",
    "We will split the precipitation images dataset into two parts: a training and a test set.\n",
    "\n",
    "The first few datasets will be taken for the training dataset and the remaining will be used for the test set.\n",
    "\n",
    "Models will be developed using the training dataset and will make predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "# Import libraries\n",
    "import os,cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import TimeDistributed\n",
    "# generate the next frame in the sequence\n",
    "\n",
    "# loading directory to generate the next frame in the sequence project\n",
    "def load_images(img_path):\n",
    "    images= glob.glob(img_path)\n",
    "    img_data = np.array([np.array(Image.open(fname)) for fname in images])\n",
    "    img_data.dump('file.npy')\n",
    "    img_data = np.load('file.npy')\n",
    "    return img_data\n",
    "\n",
    "filelist = 'dataset/precipitate_data/*.gif'\n",
    "img_data = load_images(filelist)\n",
    "#print(img_data)\n",
    "img_data = img_data.astype('float32')\n",
    "img_data /= 255\n",
    "print (img_data.shape)\n",
    "img_rows=128\n",
    "img_cols=128\n",
    "num_channel=1\n",
    "num_epoch=2\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 4\n",
    "\n",
    "if num_channel==1:\n",
    "\tif K.image_dim_ordering()=='th':\n",
    "\t\timg_data= np.expand_dims(img_data, axis=1) \n",
    "\t\tprint (img_data.shape)\n",
    "\telse:\n",
    "\t\timg_data= np.expand_dims(img_data, axis=4) \n",
    "\t\tprint (img_data.shape)\n",
    "\t\t\n",
    "else:\n",
    "\tif K.image_dim_ordering()=='th':\n",
    "\t\timg_data=np.rollaxis(img_data,3,1)\n",
    "\t\tprint (img_data.shape)\n",
    "\t\t\n",
    "#%%\n",
    "\n",
    "USE_SKLEARN_PREPROCESSING=False\n",
    "\n",
    "if USE_SKLEARN_PREPROCESSING:\n",
    "# using sklearn for preprocessing\n",
    "    from sklearn import preprocessing\n",
    "    # To resize an image, \n",
    "    def image_to_feature_vector(image, size=(128, 128)):\n",
    "        # resize the image to a fixed size, then flatten the image into\n",
    "        # a list of raw pixel intensities\n",
    "        return cv2.resize(image, size).flatten()\n",
    "    img_data_list=[]\n",
    "    img_data = dir_path(filelist)\n",
    "    print(img_data)\n",
    "    img_data_scaled = preprocessing.scale(img_data)\n",
    "    print (img_data_scaled.shape)\n",
    "\n",
    "    print (np.mean(img_data_scaled))\n",
    "    print (np.std(img_data_scaled))\n",
    "    print (img_data_scaled.mean(axis=0))\n",
    "    print (img_data_scaled.std(axis=0))\n",
    "    \n",
    "    if K.image_dim_ordering()=='th':\n",
    "        img_data_scaled=img_data_scaled.reshape(img_data.shape[0],num_channel,img_rows,img_cols)\n",
    "        print (img_data_scaled.shape)\n",
    "    else:\n",
    "        img_data_scaled=img_data_scaled.reshape(img_data.shape[0],img_rows,img_cols,num_channel)\n",
    "        print (img_data_scaled.shape)\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        img_data_scaled=img_data_scaled.reshape(img_data.shape[0],num_channel,img_rows,img_cols)\n",
    "        print (img_data_scaled.shape)\n",
    "    else:\n",
    "        img_data_scaled=img_data_scaled.reshape(img_data.shape[0],img_rows,img_cols,num_channel)\n",
    "        print (img_data_scaled.shape)\n",
    "\n",
    "        \n",
    "if USE_SKLEARN_PREPROCESSING:\n",
    "    img_data=img_data_scaled\n",
    "#%%\n",
    "# Assigning Labels\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 4\n",
    "\n",
    "num_of_samples = img_data.shape[0]\n",
    "labels = np.ones((num_of_samples,),dtype='int64')\n",
    "\n",
    "labels[0:202]=0\n",
    "labels[202:404]=1\n",
    "labels[404:606]=2\n",
    "labels[606:]=3\n",
    "\n",
    "names = ['precipitate_data']\n",
    " \n",
    "# convert class labels to on-hot encoding\n",
    "Y = np_utils.to_categorical(labels, num_classes)\n",
    "#print(Y)\n",
    "#Shuffle the dataset\n",
    "x,y = shuffle(img_data,Y, random_state=2)\n",
    "#print(x,y)\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n",
    "\n",
    "#%%\n",
    "# Defining the CNN model and create an instance of the model object from Sequential mode.\n",
    "input_shape=img_data[0].shape\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3,3,border_mode='same',input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=[\"accuracy\"])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
    "\n",
    "# Viewing model_configuration\n",
    "model.summary()\n",
    "model.get_config()\n",
    "model.layers[0].get_config()\n",
    "model.layers[0].input_shape\n",
    "model.layers[0].output_shape\n",
    "model.layers[0].get_weights()\n",
    "np.shape(model.layers[0].get_weights()[0])\n",
    "model.layers[0].trainable\n",
    "\n",
    "#%%\n",
    "# Training\n",
    "\n",
    "hist = model.fit(X_train, y_train, batch_size=16, epochs=num_epoch, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "#hist = model.fit(X_train, y_train, batch_size=32, nb_epoch=20,verbose=1, validation_split=0.2)\n",
    "\n",
    "# Training with callbacks\n",
    "from keras import callbacks\n",
    "\n",
    "filename='model_train_new.csv'\n",
    "csv_log=callbacks.CSVLogger(filename, separator=',', append=False)\n",
    "\n",
    "early_stopping=callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='min')\n",
    "\n",
    "filepath=\"Best-weights-my_model-{epoch:03d}-{loss:.4f}-{acc:.4f}.hdf5\"\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [csv_log,early_stopping,checkpoint]\n",
    "\n",
    "hist = model.fit(X_train, y_train, batch_size=16, epochs=num_epoch, verbose=1, validation_data=(X_test, y_test),callbacks=callbacks_list)\n",
    "\n",
    "# visualizing losses and accuracy\n",
    "train_loss=hist.history['loss']\n",
    "val_loss=hist.history['val_loss']\n",
    "train_acc=hist.history['acc']\n",
    "val_acc=hist.history['val_acc']\n",
    "xc=range(num_epoch)\n",
    "\n",
    "plt.figure(1,figsize=(7,5))\n",
    "plt.plot(xc,train_loss)\n",
    "plt.plot(xc,val_loss)\n",
    "plt.xlabel('num of Epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('train_loss vs val_loss')\n",
    "plt.grid(True)\n",
    "plt.legend(['train','val'])\n",
    "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "plt.style.use(['classic'])\n",
    "\n",
    "plt.figure(2,figsize=(7,5))\n",
    "plt.plot(xc,train_acc)\n",
    "plt.plot(xc,val_acc)\n",
    "plt.xlabel('num of Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('train_acc vs val_acc')\n",
    "plt.grid(True)\n",
    "plt.legend(['train','val'],loc=4)\n",
    "#print plt.style.available # use bmh, classic,ggplot for big pictures\n",
    "plt.style.use(['classic'])\n",
    "\n",
    "#%%\n",
    "\n",
    "# Evaluating the model\n",
    "\n",
    "score = model.evaluate(X_test, y_test,verbose=0)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "test_image = X_test[0:1]\n",
    "print (test_image.shape)\n",
    "\n",
    "print(model.predict(test_image))\n",
    "print(model.predict_classes(test_image))\n",
    "print(y_test[0:1])\n",
    "\n",
    "# Testing a new image\n",
    "def load_images(img_path):\n",
    "    images= glob.glob(img_path)\n",
    "    img_data = np.array([np.array(Image.open(fname)) for fname in images])\n",
    "    img_data.dump('test_file.npy')\n",
    "    img_data = np.load('test_file.npy')\n",
    "    return img_data\n",
    "file_path = 'dataset/test_data/*.gif'\n",
    "test_image = load_images(file_path)\n",
    "#print(test_image)\n",
    "test_image = test_image.astype('float32')\n",
    "test_image /= 255 #divide the pictures in \n",
    "#print(test_image)\n",
    "print (test_image.shape)\n",
    "if num_channel==1:\n",
    "\tif K.image_dim_ordering()=='th':\n",
    "\t\ttest_image= np.expand_dims(test_image, axis=0)\n",
    "\t\ttest_image= np.expand_dims(test_image, axis=0)\n",
    "\t\tprint (test_image.shape)\n",
    "\telse:\n",
    "\t\ttest_image= np.expand_dims(test_image, axis=3) \n",
    "\t\ttest_image= np.expand_dims(test_image, axis=0)\n",
    "\t\tprint (test_image.shape)\n",
    "\t\t\n",
    "else:\n",
    "\tif K.image_dim_ordering()=='th':\n",
    "\t\ttest_image=np.rollaxis(test_image,2,0)\n",
    "\t\ttest_image= np.expand_dims(test_image, axis=0)\n",
    "\t\tprint (test_image.shape)\n",
    "\telse:\n",
    "\t\ttest_image= np.expand_dims(test_image, axis=0)\n",
    "\t\tprint (test_image.shape)\n",
    "\t\t\n",
    "# Predicting the test image\n",
    "print((model.predict(test_image)))\n",
    "print(model.predict_classes(test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
